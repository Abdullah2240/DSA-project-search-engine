import json
import os
import sys

sys.stdout.reconfigure(encoding='utf-8')

# Get the folder of the current script
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

# Path to input and output files (use raw string or forward slashes)
# Use cleaned_with_body.jsonl which is generated by crawl_pdf.py and contains doc_id and url
# Falls back to cleaned.jsonl if cleaned_with_body.jsonl doesn't exist
INPUT_FILE = os.path.join(BASE_DIR, "..", "data", "processed", "cleaned_with_body.jsonl")
FALLBACK_INPUT_FILE = os.path.join(BASE_DIR, "..", "data", "processed", "cleaned.jsonl")
OUTPUT_FILE = os.path.join(BASE_DIR, "..", "data", "processed", "docid_to_url.json")

def build_docid_url_map():
    mapping = {}
    
    # Try cleaned_with_body.jsonl first, fall back to cleaned.jsonl
    input_file = INPUT_FILE if os.path.exists(INPUT_FILE) else FALLBACK_INPUT_FILE
    if not os.path.exists(input_file):
        print(f"ERROR: Neither {INPUT_FILE} nor {FALLBACK_INPUT_FILE} found!")
        print("Run getData.py and crawl_pdf.py first.")
        return
    
    print(f"Using input file: {input_file}")

    with open(input_file, "r", encoding="utf-8") as f:
        for line in f:
            if not line.strip():
                continue

            try:
                obj = json.loads(line)
            except json.JSONDecodeError:
                print("Skipping invalid line:", line[:100])
                continue

            doc_id = obj.get("doc_id")
            url = obj.get("url")

            if doc_id is None or not url:
                # Skip entries with no URL
                continue

            mapping[str(doc_id)] = url  # Store as string to avoid big JSON numbers issues

    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
    with open(OUTPUT_FILE, "w", encoding="utf-8") as out:
        json.dump(mapping, out, indent=2)

    print(f"Done! Wrote {len(mapping)} docID â†’ URL mappings to {OUTPUT_FILE}")


if __name__ == "__main__":
    build_docid_url_map()
